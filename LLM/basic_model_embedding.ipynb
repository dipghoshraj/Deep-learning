{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f618ef9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.4.1+cpu\n",
      "CUDA available: False\n",
      "CUDA version in torch: None\n",
      "Is torch built with CUDA: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version in torch:\", torch.version.cuda)\n",
    "print(\"Is torch built with CUDA:\", torch.backends.cuda.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c2b83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "459f6254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2439\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../tokenizer_data/vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0e81d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = np.memmap(\n",
    "    \"../tokenized_sql_dataset/flatten_token.memmap\",\n",
    "    dtype=np.int32,\n",
    "    mode=\"r\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9501eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, memmap_path: str, block_size: int, dtype = np.int32, start=0, end=None):\n",
    "        self.tokens =  np.memmap(memmap_path, dtype=dtype, mode='r')\n",
    "        self.block_size = block_size\n",
    "        self.total_tokens = len(self.tokens)\n",
    "\n",
    "        self.end = len(self.tokens) - block_size if end is None else end\n",
    "        self.start = start\n",
    "        self.length = self.end - self.start\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.start\n",
    "        if idx + self.block_size + 1 > self.end:\n",
    "            raise IndexError(\"Index out of bounds.\")\n",
    "        block = self.tokens[idx : idx + self.block_size + 1]\n",
    "        x = torch.tensor(block[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(block[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "    def get_batch(self, batch_size: int, device='cpu'):\n",
    "        idxs = np.random.randint(0, self.length, size=batch_size)\n",
    "        x_list, y_list = zip(*[self[i] for i in idxs])\n",
    "        x = torch.stack(x_list).to(device)\n",
    "        y = torch.stack(y_list).to(device)\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        x = torch.stack([item[0] for item in batch])\n",
    "        y = torch.stack([item[1] for item in batch])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4433376",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = len(flattened)\n",
    "n_data\n",
    "\n",
    "train_batch_size = 16  # training batch size\n",
    "eval_batch_size = 8  # evaluation batch size\n",
    "context_length = 256  # number of tokens processed in a single batch\n",
    "block_size = 256\n",
    "train_split = 0.9  # percentage of data to use from total data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47439de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_tokens = n_data - block_size\n",
    "split_ratio = 0.9\n",
    "split_index = int(usable_tokens * split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b95a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "memmap_path = \"../tokenized_sql_dataset/flatten_token.memmap\"\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        memmap_path=memmap_path,\n",
    "        block_size=block_size,\n",
    "        dtype=np.int32,\n",
    "        start=0,\n",
    "        end=split_index\n",
    "    )\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "        memmap_path=memmap_path,\n",
    "        block_size=block_size,\n",
    "        dtype=np.int32,\n",
    "        start=split_index,\n",
    "        end=usable_tokens  # to avoid index errors\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5348fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, d_model) # word token embeddings\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    # def forward(self, inputs, targets = None):\n",
    "    #     logits = self.wte(inputs) # dim -> batch_size, sequence_length, d_model\n",
    "    #     loss = None\n",
    "    #     if targets != None:\n",
    "    #         batch_size, sequence_length, d_model = logits.shape\n",
    "    #         # to calculate loss for all token embeddings in a batch\n",
    "    #         # kind of a requirement for cross_entropy\n",
    "    #         logits = logits.view(batch_size * sequence_length, d_model)\n",
    "    #         targets = targets.view(batch_size * sequence_length)\n",
    "    #         loss = F.cross_entropy(logits, targets)\n",
    "    #     return logits, loss\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        x = self.wte(inputs)                                   # [B, T, d_model]\n",
    "        logits = self.lm_head(x)                               # [B, T, vocab_size]\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B * T, V)                     # flatten for CE\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # this will store the model outputs along with the initial input sequence\n",
    "        # make a copy so that it doesn't interfare with model\n",
    "        for _ in range(max_new_tokens):\n",
    "            # we only pass targets on training to calculate loss\n",
    "            logits, _ = self(inputs)\n",
    "            # for all the batches, get the embeds for last predicted sequence\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # get the probable token based on the input probs\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            inputs = torch.cat([inputs, idx_next], dim=1)\n",
    "        # as the inputs has all model outputs + initial inputs, we can use it as final output\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2437f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = GPT(vocab_size=2439, d_model=256).to(device)\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.AdamW(basic_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a2ecec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tlr: 0.001\ttrain_loss: 7.948969841003418\teval_loss: 7.857056140899658\n",
      "Epoch: 10\tlr: 0.001\ttrain_loss: 7.146059513092041\teval_loss: 7.248891830444336\n",
      "Epoch: 20\tlr: 0.001\ttrain_loss: 6.406927108764648\teval_loss: 6.592184066772461\n",
      "Epoch: 30\tlr: 0.001\ttrain_loss: 5.628505706787109\teval_loss: 6.036105632781982\n",
      "Epoch: 40\tlr: 0.001\ttrain_loss: 5.2906413078308105\teval_loss: 5.543333530426025\n",
      "Epoch: 50\tlr: 0.001\ttrain_loss: 4.881579399108887\teval_loss: 5.243508815765381\n",
      "Epoch: 60\tlr: 0.001\ttrain_loss: 4.519382476806641\teval_loss: 4.756292343139648\n",
      "Epoch: 70\tlr: 0.001\ttrain_loss: 4.312434196472168\teval_loss: 4.8440327644348145\n",
      "Epoch: 80\tlr: 0.001\ttrain_loss: 4.198802947998047\teval_loss: 4.638958930969238\n",
      "Epoch: 90\tlr: 0.001\ttrain_loss: 3.879368543624878\teval_loss: 4.143606662750244\n",
      "Epoch: 99\tlr: 0.001\ttrain_loss: 3.4466679096221924\teval_loss: 4.285839557647705\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "eval_steps = 10 # perform evaluation in every n steps\n",
    "for ep in range(epochs):\n",
    "    xb, yb = train_loader.get_batch(train_batch_size, device)\n",
    "\n",
    "    logits, loss = basic_model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if ep % eval_steps == 0 or ep == epochs-1:\n",
    "        basic_model.eval()\n",
    "        with torch.no_grad():\n",
    "            xvb, yvb = eval_loader.get_batch(eval_batch_size, device)\n",
    "            _, e_loss = basic_model(xvb, yvb)\n",
    "\n",
    "            print(f\"Epoch: {ep}\\tlr: {lr}\\ttrain_loss: {loss}\\teval_loss: {e_loss}\")\n",
    "        basic_model.train() # back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13152837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpe.fast_token import FastBPETokenizer\n",
    "tokenizer = FastBPETokenizer()\n",
    "\n",
    "tokenizer.load(\"../tokenizer_data\")\n",
    "tokens = tokenizer.tokenize_to_ids(\"find\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c72b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = torch.tensor(tokenizer.tokenize_to_ids(\"If you want to predict/generate outputs on your model right after training without saving and loading, its very straightforward: just keep using your model instance in memory\"), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70e2d762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token IDs: tensor([ 81,  66, 220, 208,  72,  90, 126, 206,  76, 115,  99,  89, 132,  94,\n",
      "        147, 136, 132,  61, 267, 254,  94, 115, 125,   7, 230, 139, 249,  72,\n",
      "         90, 252, 230,  80, 273, 255, 206, 252, 191,  94, 212,  94, 230, 189,\n",
      "         36, 111, 104, 138,  54, 107,  36,  93,  94, 252, 198,  89, 105, 291,\n",
      "          9,  97,  72, 118, 230, 255,  94, 198,  90, 129,  89, 171, 140,  36,\n",
      "        206, 180,  89,  89, 164,  36,  36, 139, 249,  72,  90, 252, 230,  80,\n",
      "        189, 118, 126, 259, 264, 252,  90, 138,  72,  37])\n"
     ]
    }
   ],
   "source": [
    "basic_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, _ = basic_model(input_tokens)\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)  # pick the most likely token at each position\n",
    "\n",
    "print(\"Predicted token IDs:\", predicted_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "825e3f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WNoroS_id ce numberTABLE atial_dabentedId timetabatic'pearseS_id stpVut numberstlabof abpk atan e Ear a abstm_and y(agSaypt abm_id char_icec numbering__g  earseS_id stpVkayce text,the st_id e S<bos>\n"
     ]
    }
   ],
   "source": [
    "predicted_ids_list = predicted_ids.tolist()\n",
    "\n",
    "# Decode to text using your tokenizer's method\n",
    "decoded_text = tokenizer.decode_from_ids(predicted_ids_list)\n",
    "\n",
    "print(decoded_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
